---
title: "A Proposal to Improve the Hamiltonian Dynamics in Stan"
author: "Edward A. Roualdes"
date: "4/22/2020"
output:
    html_document:
        toc: true
        toc_depth: 2
bibliography: bibliography.bib
nocite: |
    @Leimkuhler:2018, @Team:2018, @Betancourt:2017
---

# Introduction {.tabset}

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

```{r imports, include=FALSE}

library(rstan)
library(ggplot2)
library(dplyr)
library(kableExtra)
library(tidyr)

theme_set(theme_minimal(base_size = 18))

```

```{r functions, include=FALSE}

read_samples <- function(dir, metric) {
  dv_samplepaths <- paste0(dir, "/", dir, "_", metric, "_dv/", "samples", 1:4, ".csv")
  pr_samplepaths <- paste0(dir, "/", dir, "_", metric, "_pr/", "samples", 1:4, ".csv")

  dv_posterior <- read_stan_csv(dv_samplepaths)
  pr_posterior <- read_stan_csv(pr_samplepaths)

  list(dv = dv_posterior, pr = pr_posterior)
}

summary_stats <- function(mntr, metric, stat) {
  parameter_names <- rownames(mntr)
  tb <- tibble(mntr)[,stat]
  tb$parameters <- parameter_names
  tb$metric <- metric
  tb$index <- 1:nrow(mntr)
  tb
}

calculate_summary_dfs <- function(dv_posterior, pr_posterior, times, metric) {

  dv_m <- monitor(dv_posterior)
  dv2_m <- monitor(as.array(dv_posterior)^2)

  pr_m <- monitor(pr_posterior)
  pr2_m <- monitor(as.array(pr_posterior)^2)


  num_parameters <- nrow(dv_m)
  parameter_names <- rownames(dv_m)

  ## ESS
  dv_nleaps <- sum(sapply(1:4, function (x) {
    attr(dv_posterior@sim$samples[[x]], "sampler_params")$n_leapfrog__}))
  pr_nleaps <- sum(sapply(1:4, function (x) {
    attr(pr_posterior@sim$samples[[x]], "sampler_params")$n_leapfrog__}))
  leaps <- c(dv_nleaps, pr_nleaps)

  dfESS <- data.frame(ess = c(dv_m$Bulk_ESS, pr_m$Bulk_ESS,
                              dv_m$Tail_ESS, pr_m$Tail_ESS),
                      ess2 = c(dv2_m$Bulk_ESS, pr2_m$Bulk_ESS,
                               dv2_m$Tail_ESS, pr2_m$Tail_ESS),
                      style = rep(c("Bulk", "Tail"), each = 2 * num_parameters),
                      Branch = rep(rep(c("develop", "proposal"), each = num_parameters), 2),
                      time = rep(times, each = 2 * num_parameters),
                      leapfrog = rep(leaps, each = 2 * num_parameters),
                      Parameters = rep(parameter_names, 4))

  ## Rhat
  dfR <- data.frame(Rhat = c(dv_m$Rhat, pr_m$Rhat),
                    Branch = rep(c("develop", "proposal"), each = num_parameters),
                    Parameters = rep(rownames(dv_m), 2))

  ## Accuracy
  mcse <- paste0("MCSE_", c("Q2.5", "Q25", "Q50", "Q75", "Q97.5", "SD"))
  ## stats <- c("2.5%", "25%", "50%", "75%", "97.5%", "mean")


  ## dfA <- bind_rows(summary_stats(dv_m, metric, stats),
  ##                  summary_stats(pr_m, metric, stats))

  dfMCSE <- bind_rows(summary_stats(dv_m, metric, mcse),
                      summary_stats(pr_m, metric, mcse))

  list(#df_stats = dfA,
    dvm = dv_m,
    prm = pr_m,
    df_ess = dfESS,
    df_rhat = dfR,
    df_mcse = dfMCSE,
    num_parameters = num_parameters,
    parameter_names = parameter_names)
}

ess_bulk_plot <- function(df) {
  df %>%
    mutate(essps = ess / time) %>%
    ggplot() +
    geom_point(aes(Parameters, essps, shape=Branch, color=Branch), size = 3) +
    facet_wrap(~style, scales="free_y") +
    labs(y = "ESS(x) per second") +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
}

ess_tail_plot <- function(df) {
  df %>%
    mutate(esspl = ess / leapfrog) %>%
    ggplot() +
    geom_point(aes(Parameters, esspl, shape=Branch, color=Branch), size = 3) +
    facet_wrap(~style, scales="free_y") +
    labs(y = "ESS(x) per leapfrog") +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
}

ess2_bulk_plot <- function(df) {
  df %>%
    mutate(ess2ps = ess2 / time) %>%
    ggplot() +
    geom_point(aes(Parameters, ess2ps, shape=Branch, color=Branch), size = 3) +
    facet_wrap(~style, scales="free_y") +
    labs(y = "ESS(x^2) per second") +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
}

ess2_tail_plot <- function(df) {
  df %>%
    mutate(ess2pl = ess2 / leapfrog) %>%
    ggplot() +
    geom_point(aes(Parameters, ess2pl, shape=Branch, color=Branch), size = 3) +
    facet_wrap(~style, scales="free_y") +
    labs(y = "ESS(x^2) per leapfrog") +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
}

rhat_plot <- function(df) {
  df %>%
    ggplot() +
    geom_point(aes(Parameters, Rhat, shape = Branch, color = Branch), size = 3) +
    geom_hline(yintercept = 1.01, linetype=2, alpha = 0.5) +
    theme(axis.text.x = element_text(angle = 30, hjust = 1))
}


parameter_plot <- function(samples_dv, dvm, samples_pr, prm) {

  dfdv_samples <- as.data.frame(samples_dv)
  dfdv_samples$Branch <- "develop"
  dfpr_samples <- as.data.frame(samples_pr)
  dfpr_samples$Branch <- "proposal"

  df <- bind_rows(dfdv_samples, dfpr_samples) %>%
    select(-lp__) %>%
    pivot_longer(cols=-Branch, names_to="parameter")

  parameter_names <- rownames(dvm)
  dvm$parameter <- parameter_names
  prm$parameter <- parameter_names
  dvm$Branch <- "develop"
  prm$Branch <- "proposal"
  stats <- c("2.5%", "25%", "50%", "75%", "97.5%", "Branch", "parameter")

  dfQ <- bind_rows(dvm, prm) %>%
    tibble %>%
    select(all_of(stats)) %>%
    filter(parameter != "lp__") %>%
    pivot_longer(cols=-c(Branch, parameter), names_to="quantile")


  ggplot() +
    geom_density(data = df, aes(value, color=Branch)) +
    geom_rug(data = dfQ, aes(value, color=Branch), size=2) +
    facet_wrap(~parameter, scales="free", ncol=1)
}
```

I propose new dynamics for the Hamiltonian Monte Carlo (HMC) algorithm
implemented in the probabilistic programming language
[Stan](https://mc-stan.org/).  This technical report compares Stan's
version of HMC to my proposal, but does not attempt to explain
Hamiltonian Monte Carlo.  For a thorough introduction to HMC, consider
Michael Betancourt's [A Conceptual Introduction to Hamiltonian Monte
Carlo](https://arxiv.org/abs/1701.02434).

After describing the differences between these two versions of HMC, a
simulation study is carried out.  XXX Describe the simulation study:
what are we comparing and why.  Many of the models used in this study
can be found in the GitHub repository
[stan-dev/performance-tests-cmdstan](https://github.com/stan-dev/performance-tests-cmdstan)
and specifically in the folder stat_comp_benchmarks therein.  A few
variations on Gaussian and studten-t distributions are also
considered.

# Proposed Dynamics {.tabset}

Let $\pi(q) \propto \exp{(-U(q))}$ be the target distribution of
interest for parameters $q \in \mathbb{R}^{d}$.  As of
2020-04-22, the [Stan GitHub branch](https://github.com/stan-dev/stan)
develop's version of HMC could be written as a Hamiltonian

$$ H_{dv}(q, p) = U(q) + \frac{1}{2} p^T M^{-1} p $$

where the subscript $dv$ stands for develop, $M^{-1} \approx
\mathbb{E}_{\pi} (q - \mu)(q - \mu)^T$.  Under this scheme, $p \sim
N(0, M)$.

The work of @Ma:2015 in [A Complete Recipe for Stochastic Gradient
MCMC](https://arxiv.org/abs/1506.04696) suggests a richer framework
than shown below.  They also offer a stochastic version of this, but
I'm ignoring all of that.  In an effort to simplify Ma's notation,
I've removed some terms from the equations below that will drop out
eventually anyway of what I'm proposing.

Let $z = (q, p)$.  Ma et al.'s framework dictates dynamics as

$$\mathrm{d}z =  -Q(z) \nabla H(z)$$

where $Q$ is any skew symmetric matrix.  Some choices of $Q$ will lead
to faster convergence than others.

Branch develop's version of HMC, in the notation of this framework,
goes like this.  Draw $p \sim N(0, M)$, necessitating a
decomposition $LL^{T} = M^{-1}$ and a solve().  This was chosen by
design because $M^{-1}$ itself is needed in the next step, and it's
expensive to keep both $L$ and $M^{-1}$ in memory.  With $p$ in hand
and

$$Q(z) = \begin{bmatrix} 0 && -I \\ I && 0 \end{bmatrix}$$

update $z$ according to the dynamcis

$$ \mathrm{d}z = -Q(z) \nabla U(q) = \begin{bmatrix} 0 && -I \\ I && 0 \end{bmatrix} \begin{bmatrix} \nabla U(q) \\ M^{-1}p \end{bmatrix} = \begin{bmatrix} M^{-1}p \\ -\nabla U(q) \end{bmatrix}.$$

Stan appproximates these dynamics using the half-step [leapfrog
integrator](https://en.wikipedia.org/wiki/Leapfrog_integration) and
this strategy will apply throughout this discussion.

The proposed version of HMC draws $p_t \sim N(0, I)$, thus specifying
a slightly modified Hamiltonian,

$ $H_{pr}(z) = U(q) + \frac{1}{2}p^Tp.$$

Take

$$ Q(z) = \begin{bmatrix} 0 && -L^T \\ L && 0 \end{bmatrix} $$

where $LL^T = M^{-1}$.  Since $M^{-1}$ is only updated at the end of
each [adaption window](https://mc-stan.org/docs/2_23/reference-manual/hmc-algorithm-parameters.html#adaptation.figure), $L$ is fixed throughout sampling.  There is no
need for $M^{-1}$ itself. The proposed dynamics then follow

$$ \mathrm{d}z = \begin{bmatrix} L^{T}p \\ -L \nabla U(q) \end{bmatrix}.$$

XXX Consider the one step update to see the similarity between the two models.

$$ q(t + \epsilon) = q(t) + LL^T... $$

Then note how similar these methods are in terms of $p$ as a random
variable.  In develop, we have $p \sim N(0, M)$ and hence in the from
the one step we have $M^{-1}p$.  In terms of $LL^T = M^{-1}$, the
distribution of $M^{-1}p \sim N(0, LL^T)$.  Similarly, for proposal
$Lp \sim N(0, LL^T)$.

The last (ha!) change necessary is to measure distance appropriately.
In develop, and using the notation of @Betancourt:2017,

$$ q_+(\mathfrak{t}) - q_-(\mathfrak{t}) = \int_{t = 0}^{t =
T(\mathfrak{t})} \mathrm{d}t M^{-1} \cdot p(t). $$

Since proposal generates $p \sim N(0, 1)$ instead of $p \sim N(0, M)$,
replace $M^{-1}$ with $I$, the identity matrix of an appropriate size.
This ensures distance is measured with respect to the appropriate
metric.

Overall, the proposed dynamics should be faster.  There are fewer
decompositions and there is no matrix inverse via a call to solve().
While $L \cdot \nabla U(q)$ is calculated twice within the leapfrog
integrator, once for each half-step, the matrix vector multiplication
should be more efficient in Eigen since $L$ is diagonal^[Thanks to
Matija Rezar and Ben Bales for pointing this out.].  Moreover, by
measuring distance with respect to an identity metric instead of with
respect to $M^{-1}$, the calculation $M^{-1}p$, which happens number
of leapfrog updates + 1 for every transition, is completely dropped.


# Numerical Study {.tabset}

Below is a comparison of the two version of HMC dynamics presented
above, for both XXX change this metrics dense\_e and diag\_e across a range of models.
All models were fit with CmdStan v2.22.1 on a MacBook Pro running
macOS 10.14.6, which has a 2GHz Intel Core i5 processor and 16GB 1867
MHz LPDDR3 memory.  Each model was fit with 2000 warmup iterations,
10000 iterations beyond warmup, and 4 chains.  All other Stan defaults
were left alone.

XXX Repeat purpose of study and state where proposed code can be found.

XXX Describe study
leapfrog steps summed across all four chains.

XXX preparation for results below. some models in stat_comp_benchmark
were funky and I excluded them.  I tried to increase adapt_delta and
max_depth, but if there was still considerable variation in the
fitting time for either proposal or develop, then I just ignore that
model.  Tried to first increase adapt delta from 0.8 to 0.9, and if
needed from 0.9 to 0.95, but no higher.  If that didn't fix either
stepsize or max tree depth issues, then I'd adjust max_depth from 10
to 12, but no higher.  If there were still issues, like order of
magnitude different fitting times, then I threw the model out.  If
modifications to adapt_delta and/or max_treedepth fixed algorithmic
issues for one branch, then these same changes were made to the other
branch.  Such updates proceeded until both algorithms were relatively
stable.  Some stable runs included less than 1% divergences, ignoring
these as false positives.

XXX time measured as the total time to fit 4 chains as if run sequentially




## gauss_c25

```{r, include=FALSE, cache=TRUE}
agc25_times <- c(116, 51)
agc25_samples <- read_samples("anisotropic_gauss_c25_100d", "dense")
agc25 <- calculate_summary_dfs(agc25_samples$dv, agc25_samples$pr, agc25_times, "dense_e")

```

### Runtime

The model agc25 has `r agc25$num_parameters` parameters.  The table below displays the run time of this model in seconds.

```{r, echo=FALSE}

dft <- data.frame(metric = c("develop", "proposal"),
                  seconds = agc25_times)
kable(dft, col.names = c("Branch", "seconds")) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE, full_width = FALSE)

```



### ESS

```{r, echo=FALSE, fig.width = 12}

ess_bulk_plot(agc25$df_ess)
ess_tail_plot(agc25$df_ess)

```

```{r, echo=FALSE, fig.width = 12}

ess2_bulk_plot(agc25$df_ess)
ess2_tail_plot(agc25$df_ess)

```

### Rhat

```{r, echo=FALSE}

rhat_plot(agc25$df_rhat)

```

### Summary Statistics

Estimated parameters.

```{r, echo=FALSE, fig.height=200, fig.width = 8, cache=TRUE}

parameter_plot(agc25_samples$dv, agc25$dvm, agc25_samples$pr, agc25$prm)

```

Parameter Monte Carlo standard errors.

```{r, echo=FALSE}

agc25$df_mcse[order(agc25$df_mcse$index), ] %>%
    select(metric, everything()) %>%
    select(-parameters, -index) %>%
    kable(digits = 3) %>%
    pack_rows(index = table(agc25$df_mcse$parameters)) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE)

```


## Model: arK

```{r, include=FALSE, cache=TRUE}

arK_samples <- read_samples("ark", "dense")
arK <- calculate_summary_dfs(arK_samples$dv, arK_samples$pr, c(11, 87), "dense_e")

```

### Runtime

The model arK has `r arK$num_parameters` parameters.  The table below displays the run time of this model in seconds.

```{r, echo=FALSE}

times <- c(6, 11)
dft <- data.frame(metric = c("develop", "proposal"),
                  seconds = times)
kable(dft, col.names = c("Branch", "seconds")) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE, full_width = FALSE)

```



### ESS

```{r, echo=FALSE, fig.width = 10}

ess_bulk_plot(arK$df_ess)
ess_tail_plot(arK$df_ess)

```

```{r, echo=FALSE, fig.width = 10}

ess2_bulk_plot(arK$df_ess)
ess2_tail_plot(arK$df_ess)

```

### Rhat

```{r, echo=FALSE}

rhat_plot(arK$df_rhat)

```

### Summary Statistics

Estimated parameters.

```{r, echo=FALSE, fig.height=16, fig.width = 8}

parameter_plot(arK_samples$dv, arK$dvm, arK_samples$pr, arK$prm)

```

Parameter Monte Carlo standard errors.

```{r, echo=FALSE}

arK$df_mcse[order(arK$df_mcse$index), ] %>%
    select(metric, everything()) %>%
    select(-parameters, -index) %>%
    kable(digits = 3) %>%
    pack_rows(index = table(arK$df_mcse$parameters)) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE)

```

## Model: arma

```{r, include=FALSE, cache=TRUE}
arma_times <- c(3, 3)
arma_samples <- read_samples("arma", "dense")
arma <- calculate_summary_dfs(arma_samples$dv, arma_samples$pr, arma_times, "dense_e")

```

### Runtime

The model arma has `r arma$num_parameters` parameters.  The table below displays the run time of this model in seconds.

```{r, echo=FALSE}

dft <- data.frame(metric = c("develop", "proposal"),
                  seconds = arma_times)
kable(dft, col.names = c("Branch", "seconds")) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE, full_width = FALSE)

```



### ESS

```{r, echo=FALSE, fig.width = 10}

ess_bulk_plot(arma$df_ess)
ess_tail_plot(arma$df_ess)

```

```{r, echo=FALSE, fig.width = 10}

ess2_bulk_plot(arma$df_ess)
ess2_tail_plot(arma$df_ess)

```

### Rhat

```{r, echo=FALSE}

rhat_plot(arma$df_rhat)

```

### Summary Statistics

Estimated parameters.

```{r, echo=FALSE, fig.height=16, fig.width = 8}

parameter_plot(arma_samples$dv, arma$dvm, arma_samples$pr, arma$prm)

```

Parameter Monte Carlo standard errors.

```{r, echo=FALSE}

arma$df_mcse[order(arma$df_mcse$index), ] %>%
    select(metric, everything()) %>%
    select(-parameters, -index) %>%
    kable(digits = 3) %>%
    pack_rows(index = table(arma$df_mcse$parameters)) %>%
    kable_styling(bootstrap_options = c("striped", "hover"), fixed_thead = TRUE)

```

# References {.tabset}

---
title: "A Proposal to Improve the Hamiltonian Dynamics in Stan"
author: "Edward A. Roualdes"
date: "4/22/2020"
output:
    html_document:
        toc: true
        toc_depth: 2
bibliography: bibliography.bib
nocite: |
    @Leimkuhler:2018, @Team:2018, @Betancourt:2017
---

# Introduction

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

```

I propose new dynamics for the Hamiltonian Monte Carlo (HMC) algorithm
implemented in the probabilistic programming language
[Stan](https://mc-stan.org/).  This technical report compares Stan's
version of HMC to my proposal, but does not attempt to explain
Hamiltonian Monte Carlo.  For a thorough introduction to HMC, consider
Michael Betancourt's [A Conceptual Introduction to Hamiltonian Monte
Carlo](https://arxiv.org/abs/1701.02434).

After describing the differences between these two versions of HMC, a
simulation study is carried out.  XXX Describe the simulation study:
what are we comparing and why.  Many of the models used in this study
can be found in the GitHub repository
[stan-dev/performance-tests-cmdstan](https://github.com/stan-dev/performance-tests-cmdstan)
and specifically in the folder stat_comp_benchmarks therein.  A few
variations on Gaussian and studten-t distributions are also
considered.

# Proposed Dynamics {.tabset}

Let $\pi(q) \propto \exp{(-U(q))}$ be the target distribution of
interest for parameters $q \in \mathbb{R}^{d}$.  As of
2020-04-22, the [Stan GitHub branch](https://github.com/stan-dev/stan)
develop's version of HMC could be written as a Hamiltonian

$$ H_{dv}(q, p) = U(q) + \frac{1}{2} p^T M^{-1} p $$

where the subscript $dv$ stands for develop and $M^{-1} \approx
\mathbb{E}_{\pi} (q - \mu)(q - \mu)^T$.  Under this scheme, $p \sim
N(0, M)$.


The work of @Ma:2015 in [A Complete Recipe for Stochastic Gradient
MCMC](https://arxiv.org/abs/1506.04696) suggests a richer framework
than presented below.  They also offer a stochastic version of their
framework, but the stochastics is of no interest here.  In an effort
to simplify Ma's notation, some of their terms, that will drop out
from this proposal eventually anyway, have been removed.

Let $z = (q, p)$.  Ma et al.'s framework dictates dynamics as

$$\mathrm{d}z = -Q(z) \nabla H(z)$$

where $Q$ is any skew symmetric matrix. Some choices of $Q$ will lead
to faster convergence than others.  This proposal maintains $M^{-1}$
as a pre-conditioner, but incorporates it in a new way.

Branch develop's version of HMC, in the notation of Ma's framework,
goes like this.  Draw $p \sim N(0, M)$, necessitating a
decomposition $LL^{T} = M^{-1}$ and a solve().  This was chosen by
design because $M^{-1}$ itself is needed in the next step, and it's
expensive to keep both $L$ and $M^{-1}$ in memory.  With $p$ in hand
and

$$Q(z) = \begin{bmatrix} 0 && -I \\ I && 0 \end{bmatrix}$$

update $z$ according to the dynamcis

$$ \mathrm{d}z = -Q(z) \nabla U(q) = \begin{bmatrix} 0 && -I \\ I && 0 \end{bmatrix} \begin{bmatrix} \nabla U(q) \\ M^{-1}p \end{bmatrix} = \begin{bmatrix} M^{-1}p \\ -\nabla U(q) \end{bmatrix}.$$

Stan appproximates the dynamics of this Hamiltonian system in time $t$
using the [leapfrog
integrator](https://en.wikipedia.org/wiki/Leapfrog_integration), which
moves in steps of size $\epsilon$.

\begin{align}
    p_{t + \epsilon / 2} & = p_{t} - \frac{\epsilon}{2}\nabla U(q_{t}) \\
    q_{t + \epsilon} & = q_{t} + \epsilon M^{-1}p_{t + \epsilon / 2} \\
    p_{t + \epsilon} & = p_{t + \epsilon / 2} - \frac{\epsilon}{2} \nabla U(q_{t + \epsilon})
\end{align}

Combining the first two steps into one and writing $LL^T = M^{-1}$, we see that $q$ is updated as

$$q_{t + \epsilon} = q_{t} + \epsilon LL^T p_{t} - \frac{\epsilon^2}{2} LL^T \nabla U(q_{t}) $$

where $p \sim N(0, (LL^T)^{-1})$ implies that $LL^Tp \sim N(0, LL^T)$.

Now consider the proposed version of HMC, where $p \sim N(0, I)$
gives a slightly different Hamiltonian,

$$H_{pr}(z) = U(q) + \frac{1}{2}p^Tp.$$

Take

$$ Q(z) = \begin{bmatrix} 0 && -L^T \\ L && 0 \end{bmatrix} $$

where $LL^T = M^{-1}$.  Since $M^{-1}$ is only updated at the end of
each [adaption
window](https://mc-stan.org/docs/2_23/reference-manual/hmc-algorithm-parameters.html#adaptation.figure),
$L$ is fixed throughout sampling.  There is no need for $M^{-1}$
itself, not even to generate new momenta $p$. Hence there is no solve()
under this proposed scheme.  The rest of this write up ensures that
the other pieces of the Hamiltonian Monte Carlo algorithm are updated
apppropriately to the modified Hamiltonian just presented.

The proposed dynamics then follow

$$ \mathrm{d}z = \begin{bmatrix} L^{T}p \\ -L \nabla U(q) \end{bmatrix}.$$

Using the same leapfrog integrator, the one step update of $q$ is

$$ q_{t + \epsilon} = q_{t} + \epsilon L p_{t} - \frac{\epsilon^2}{2}LL^T \nabla U(q_{t}). $$

Under this proposal, since $p \sim N(0, I)$, we have that $Lp \sim
N(0, LL^T)$ just as in the one step update of $q$ under branch
develop.

The last necessary change is to measure distance appropriately under the Hamiltonian.
In develop, and using the notation of @Betancourt:2017,

$$ q_+(\mathfrak{t}) - q_-(\mathfrak{t}) = \int_{t = 0}^{t =
T(\mathfrak{t})} \mathrm{d}t M^{-1} \cdot p(t). $$

Under the proposal, replace $M^{-1}$ with $I$, since $p \sim N(0, I)$
instead of $p \sim N(0, M)$.  This ensures distance is measured with
respect to the appropriate metric.

XXX but do they take the same number of leapfrog steps, or does the
metric $M^{-1}$ somehow measure distance better with respect to the
target $\pi$?  If this isn't true, then we need to completely rethink
the following paragraph.

Overall, the proposed dynamics should be faster and more memory
efficient to sample from $\pi$.  However, any noticeable difference
depends on the time spent integrating, and evaluating $U$ and $\nabla
U$.

Under the proposed scheme, there is no decomposition nor solve(),
which otherwise occur once for each transition at a cost of $d^3 / 3$
and $d^2$ operations respectively.  With no solve(), the proposal
should also consume less memory.

While $L \nabla U(q)$ is calculated twice within the leapfrog
integrator, once for each half-step, this matrix-vector multiplication
can be made more efficient in
[Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page) since
$L$ is lower triangular^[Thanks to Matija Rezar and Ben Bales for
pointing this out.].  Since both proposal and develop must evaluate
the gradient of $U$ at $q$, let's write $\nabla U$ to emphasize that
we can ignore this evaluation cost in a direct comparison of necessary
operations.  Beyond the evaluation of $\nabla U$, the difference in
computational cost for each leapfrog integration between proposal and
develop comes down to a comparison between the cost of evaluating $L
\nabla U$ twice and $M^{-1}p$ once.  For proposal, because $L$ is
lower triangular, there are $d(d+1) / 2 + (d-1)d / 2$ operations for
each $L \nabla U$ calculation, for a total of $2d^2$ operations.  For
develop, since $M^{-1}$ is dense, this is a dense matrix with $d^2$
elements multiplied by a vector $p$ of lenth $d$, for a computational
cost of $2d^2 - d$. Essentially, calculating $L \nabla U$ twice
entails traversing the diagonal of a $d \times d$ matrix one extra
time for each leapfrog iteration.

Next, consider the metric under which distance is measured within the
two branches.  By measuring distance with respect to an identity
metric instead of with respect to $M^{-1}$, the calculation $M^{-1}p$,
which happens number of leapfrog + 1 for every transition $i = 1:I$,
is replaced with $p^Tp$.  Assuming $K$ leapfrog steps happen on
average, then proposal saves $I * (K + 1) * (2d^2 - 3d + 1)$
operations.




# Numerical Study {.tabset}

Below is a comparison of the two version of HMC dynamics presented
above, for both XXX change this metrics dense\_e and diag\_e across a
range of models.  All models were fit with CmdStan v2.22.1 on a
MacBook Pro running macOS 10.14.6, which has a 2GHz Intel Core i5
processor and 16GB 1867 MHz LPDDR3 memory.  Each model was fit with
2000 warmup iterations, 10000 iterations beyond warmup, and 4 chains.
All other Stan defaults were left alone.

XXX Repeat purpose of study and state where proposed code can be found.

XXX Describe study
leapfrog steps summed across all four chains.

XXX preparation for results below. some models in stat_comp_benchmark
were funky and I excluded them.  I tried to increase adapt_delta and
max_depth, but if there was still considerable variation in the
fitting time for either proposal or develop, then I just ignore that
model.  Tried to first increase adapt delta from 0.8 to 0.9, and if
needed from 0.9 to 0.95, but no higher.  If that didn't fix either
stepsize or max tree depth issues, then I'd adjust max_depth from 10
to 12, but no higher.  If there were still issues, like order of
magnitude different fitting times, then I threw the model out.  If
modifications to adapt_delta and/or max_treedepth fixed algorithmic
issues for one branch, then these same changes were made to the other
branch.  Such updates proceeded until both algorithms were relatively
stable.  Some stable runs included less than 1% divergences, ignoring
these as false positives.

XXX time measured as the total time to fit 4 chains as if run sequentially



# References
